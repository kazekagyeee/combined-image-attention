"""
vlm_detector_pipeline.py

–§—É–Ω–∫—Ü–∏–∏:
1) –¥–µ—Ç–µ–∫—Ç–∏—Ç —Å –ø–æ–º–æ—â—å—é VLM (OWL-ViT –∏–ª–∏ Grounding DINO, –æ–ø—Ü–∏—è –≤ config)
2) —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–µ bbox –∫–∞–∫ –ø–æ–¥–∫–∞—Ä—Ç–∏–Ω–∫–∏
3) –¥–µ–ª–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫ –ø–æ–¥–∫–∞—Ä—Ç–∏–Ω–∫–∞–º —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—É—é VLM (captioning)
4) –ø–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–¥–∫–∞—Ä—Ç–∏–Ω–æ–∫ –ü–û –¢–ï–ö–°–¢–û–í–û–ú–£ –û–ü–ò–°–ê–ù–ò–Æ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON

–ó–∞–ø—É—Å–∫:
python vlm_detector_pipeline.py --model owlvit --input_dir ./images --out_dir ./out --prompt "a dog, a cat"
"""

import os
import json
import argparse
from pathlib import Path
from PIL import Image
from tqdm import tqdm

import torch
import torchvision.transforms as T

# transformers for OWL-ViT (object detection) and BLIP (captioning)
from transformers import OwlViTProcessor, OwlViTForObjectDetection
from transformers import BlipProcessor, BlipForConditionalGeneration

# sentence-transformers for textual embeddings
from sentence_transformers import SentenceTransformer

# try optional GroundingDINO import (user may need to install separately)
HAS_GROUNDING_DINO = False
try:
    # If groundingdino package or local module is installed, import predictor
    # from groundingdino.predictor import Predictor  # common in many forks
    # but names vary by repo; we'll attempt a couple
    from groundingdino.models import build_model as gd_build_model  # type: ignore
    HAS_GROUNDING_DINO = True
except Exception:
    # Not installed ‚Äî it's optional. We'll fallback to OWL-ViT.
    HAS_GROUNDING_DINO = False


# ---------------------------
# Helpers
# ---------------------------
def ensure_dir(p: str):
    Path(p).mkdir(parents=True, exist_ok=True)

def save_crop(img: Image.Image, bbox, dest_path: str):
    # bbox = (x_min, y_min, x_max, y_max) in pixels
    x0, y0, x1, y1 = map(int, bbox)
    crop = img.crop((x0, y0, x1, y1))
    crop.save(dest_path)
    return dest_path, crop.size  # return path and (w,h)


# ---------------------------
# Detectors (abstract + implementations)
# ---------------------------

class DetectorBase:
    def __init__(self, device='cpu'):
        self.device = device


class OwlViTDetector(DetectorBase):
    def __init__(self, model_name='google/owlvit-base-patch32', device='cpu'):
        super().__init__(device)
        print(f"üîç Loading OWL-ViT model: {model_name} ...")
        self.processor = OwlViTProcessor.from_pretrained(model_name)
        self.model = OwlViTForObjectDetection.from_pretrained(model_name).to(device)

    def detect(self, image: Image.Image, text_queries: list, box_threshold=0.3):
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫:
        [
            {"bbox": (x0, y0, x1, y1), "score": float, "label": "cat" (or text query matched)}
        ]
        """

        # 1Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
        inputs = self.processor(text=text_queries, images=image, return_tensors="pt").to(self.model.device)

        # 2Ô∏è‚É£ –ü—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
        with torch.no_grad():
            outputs = self.model(**inputs)

        # 3Ô∏è‚É£ –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
        target_sizes = torch.tensor([image.size[::-1]]).to(self.model.device)  # (H, W)
        results = self.processor.post_process_object_detection(
            outputs=outputs,
            threshold=box_threshold,
            target_sizes=target_sizes
        )[0]

        # 4Ô∏è‚É£ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–π
        detections = []
        for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
            detections.append({
                "bbox": tuple(box.tolist()),           # –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (x0, y0, x1, y1)
                "score": float(score.item()),           # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                "label": text_queries[label] if label < len(text_queries) else str(label)  # —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            })

        return detections


class OwlViTDetector(DetectorBase):
    def __init__(self, model_name='google/owlvit-base-patch32', device='cpu'):
        super().__init__(device)
        print("Loading OWL-ViT:", model_name)
        self.processor = OwlViTProcessor.from_pretrained(model_name)
        self.model = OwlViTForObjectDetection.from_pretrained(model_name).to(device)

    def detect(self, image: Image.Image, text_queries: list, box_threshold=0.3):
        # prepare queries prompt as list of strings
        inputs = self.processor(text=text_queries, images=image, return_tensors="pt").to(self.model.device)
        outputs = self.model(**inputs)

        # convert outputs (following HF OwlViT example)
        target_sizes = torch.tensor([image.size[::-1]]).to(self.model.device)  # (H,W)
        results = self.processor.post_process_object_detection(outputs=outputs, threshold=box_threshold, target_sizes=target_sizes)[0]
        detections = []
        for score, label, box in zip(results["scores"].tolist(), results["labels"].tolist(), results["boxes"].tolist()):
            label_str = self.model.config.id2label[label] if hasattr(self.model.config, "id2label") else str(label)
            # boxes are [x0, y0, x1, y1] in pixels
            detections.append({"bbox": tuple(box), "score": float(score), "label": label_str})
        return detections


class GroundingDINOPlaceholder(DetectorBase):
    """
    –ó–∞–≥–ª—É—à–∫–∞/—Å–∫–µ–ª–µ—Ç –¥–ª—è Grounding DINO.
    –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—É—é Grounding DINO, —É—Å—Ç–∞–Ω–æ–≤–∏ –µ—ë —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ –∏–º–ø–æ—Ä—Ç—ã,
    –∑–∞—Ç–µ–º –∑–∞–º–µ–Ω–∏ –ª–æ–≥–∏–∫—É –≤ detect() –Ω–∞ –≤—ã–∑–æ–≤—ã –∏–∑ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ GroundingDINO.predictor.
    """
    def __init__(self, device='cpu'):
        super().__init__(device)
        if not HAS_GROUNDING_DINO:
            raise RuntimeError("Grounding DINO –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏ repo IDEA-Research/GroundingDINO –∏ –ø–æ–≤—Ç–æ—Ä–∏.")
        # TODO: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏

    def detect(self, image: Image.Image, text_queries: list, box_threshold=0.3):
        # TODO: –∑–∞–º–µ–Ω–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–º –≤—ã–∑–æ–≤–æ–º Grounding DINO
        raise NotImplementedError("–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Grounding DINO. –°–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑—É–π OWL-ViT –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏ Grounding DINO.")


# ---------------------------
# Captioning and Embeddings
# ---------------------------
class Captioner:
    def __init__(self, model_name="Salesforce/blip-image-captioning-base", device='cpu'):
        print("Loading captioning model:", model_name)
        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)
        self.device = device

    def describe(self, pil_image: Image.Image, max_length=32):
        inputs = self.processor(images=pil_image, return_tensors="pt").to(self.device)
        out = self.model.generate(**inputs, max_new_tokens=max_length)
        caption = self.processor.decode(out[0], skip_special_tokens=True)
        return caption


class TextEmbedder:
    def __init__(self, model_name="all-MiniLM-L6-v2", device='cpu'):
        print("Loading text embedder:", model_name)
        self.model = SentenceTransformer(model_name, device=device)

    def embed(self, texts: list):
        return self.model.encode(texts, convert_to_numpy=True)


# ---------------------------
# Pipeline
# ---------------------------
def process_image(path, detector, captioner, embedder, out_dir, prompt_queries):
    img = Image.open(path).convert("RGB")
    detections = detector.detect(img, prompt_queries)

    # if no detections, return empty
    if not detections:
        return []

    # sort by score descending
    detections = sorted(detections, key=lambda x: x["score"], reverse=True)

    # compute areas and coefficients
    areas = []
    for d in detections:
        x0, y0, x1, y1 = d["bbox"]
        w = max(0, x1 - x0); h = max(0, y1 - y0)
        areas.append(w * h)
    total_area = sum(areas) if sum(areas) > 0 else 1.0
    coeffs = [a / total_area for a in areas]

    items = []
    base_name = Path(path).stem
    for i, (d, coeff) in enumerate(zip(detections, coeffs)):
        bbox = d["bbox"]
        score = d["score"]
        label = d.get("label", "")
        crop_name = f"{base_name}_crop_{i}.jpg"
        crop_path = os.path.join(out_dir, crop_name)
        _, (w,h) = save_crop(img, bbox, crop_path)

        # caption the crop
        caption = captioner.describe(Image.open(crop_path).convert("RGB"))

        items.append({
            "crop_path": os.path.abspath(crop_path),
            "orig_path": os.path.abspath(path),
            "bbox": [float(x) for x in bbox],
            "score": float(score),
            "label": label,
            "caption": caption,
            "area": float(areas[i]),
            "rel_size_coeff": float(coeff),
            "crop_wh": [w, h]
        })

    # compute embeddings for all captions
    captions = [it["caption"] for it in items]
    embeddings = embedder.embed(captions)
    for it, emb in zip(items, embeddings):
        it["text_embedding"] = emb.tolist()  # numpy -> list to save to json
    return items


def run_folder(input_dir, out_dir, detector_backend="owlvit", prompt="person, dog, cat", device="cpu", json_out="metadata.json"):
    ensure_dir(out_dir)
    # init detector
    if detector_backend.lower() == "owlvit":
        detector = OwlViTDetector(device=device)
    elif detector_backend.lower() == "groundingdino":
        if not HAS_GROUNDING_DINO:
            raise RuntimeError("Grounding DINO –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏ –µ–≥–æ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π owlvit.")
        detector = GroundingDINOPlaceholder(device=device)
    else:
        raise ValueError("Unknown detector backend: choose 'owlvit' or 'groundingdino'")

    captioner = Captioner(device=device)
    embedder = TextEmbedder(device=device)

    all_metadata = []
    image_files = list(Path(input_dir).glob("*"))
    image_files = [str(p) for p in image_files if p.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp"]]

    prompt_queries = [q.strip() for q in prompt.split(",") if q.strip()]

    for img_path in tqdm(image_files, desc="Processing images"):
        items = process_image(img_path, detector, captioner, embedder, out_dir, prompt_queries)
        # append items
        all_metadata.extend(items)

    # normalize rel_size_coeff to sum=1 across all found crops (required by user)
    # current pipeline computed per-image coeffs; user asked sum of coefficients = 1 across obtained subimages.
    # We'll normalize across ALL found items.
    if all_metadata:
        total_coeff = sum(item["rel_size_coeff"] for item in all_metadata)
        if total_coeff > 0:
            for item in all_metadata:
                item["rel_size_coeff"] = float(item["rel_size_coeff"] / total_coeff)

    # save metadata
    out_json = os.path.join(out_dir, json_out)
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(all_metadata, f, ensure_ascii=False, indent=2)

    print(f"Saved metadata to {out_json} ‚Äî {len(all_metadata)} crops total.")
    return all_metadata


# ---------------------------
# CLI
# ---------------------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--model", choices=["owlvit", "groundingdino"], default="owlvit", help="detector backend")
    p.add_argument("--input_dir", required=True, help="folder with images")
    p.add_argument("--out_dir", required=True, help="output folder for crops and metadata")
    p.add_argument("--prompt", default="person, dog, cat", help="comma-separated object queries")
    p.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu", help="cpu or cuda")
    p.add_argument("--json", default="metadata.json", help="json filename for metadata")
    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()
    run_folder(args.input_dir, args.out_dir, detector_backend=args.model, prompt=args.prompt, device=args.device, json_out=args.json)
